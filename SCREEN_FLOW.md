# Data Profiling Utility - Screen Flow & Functionalities

## Overview
The Data Profiling Utility consists of 5 main screens designed for CSV file profiling with a simple, intuitive workflow.

---

## Screen Navigation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Top Navigation Bar                      â”‚
â”‚  [Data Profiling Utility] | Jobs | History | Settings       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. HOME/LANDING PAGE
   â”‚
   â”œâ”€ Quick stats (total jobs, recent activity)
   â”œâ”€ Recent profiling jobs list
   â”œâ”€ Quick links to recent datasets
   â”‚
   â”œâ”€â†’ [+ New Profiling Job] Button
   â”‚   â”‚
   â”‚   â””â”€â†’ 2. CONFIGURATION SCREEN (Multi-step Wizard)
   â”‚       â”‚
   â”‚       â”œâ”€ STEP 1: Data Source Setup
   â”‚       â”‚  â€¢ Select connection type (Flat File for CSV)
   â”‚       â”‚  â€¢ File upload interface with drag-and-drop
   â”‚       â”‚  â€¢ Multi-file selection
   â”‚       â”‚  â””â”€â†’ [Next] button
   â”‚       â”‚
   â”‚       â”œâ”€ STEP 2: Dataset & Entity Selection
   â”‚       â”‚  â€¢ Job Name input (required)
   â”‚       â”‚  â€¢ Uploaded files list with parsing config
   â”‚       â”‚    - Delimiter, header row settings per file
   â”‚       â”‚  â€¢ [Add More Files] button
   â”‚       â”‚  â””â”€â†’ [Back] [Next] buttons
   â”‚       â”‚
   â”‚       â”œâ”€ STEP 3: Profiling Options
   â”‚       â”‚  â€¢ Profile all columns or select specific
   â”‚       â”‚  â€¢ Sample size configuration
   â”‚       â”‚  â””â”€â†’ [Back] [Start Profiling] buttons
   â”‚       â”‚
   â”‚       â””â”€â†’ [Start Profiling] Button
   â”‚           â”‚
   â”‚           â””â”€â†’ 3. DATASET PROFILE DASHBOARD
   â”‚               â”‚
   â”‚               â”œâ”€ Dataset-Level Summary Cards
   â”‚               â”‚  â€¢ Total Entities, Avg Quality, Total Rows/Columns
   â”‚               â”‚  â€¢ Overall quality grade badge
   â”‚               â”‚
   â”‚               â”œâ”€ Data Quality Visualizations
   â”‚               â”‚  â€¢ Quality Grade Distribution Chart
   â”‚               â”‚  â€¢ Entity Quality Scores Chart
   â”‚               â”‚
   â”‚               â”œâ”€ Entity List/Grid
   â”‚               â”‚  â€¢ Summary cards per entity
   â”‚               â”‚  â€¢ Search, filter, sort capabilities
   â”‚               â”‚
   â”‚               â””â”€â†’ [Click Entity Card/Row]
   â”‚                   â”‚
   â”‚                   â””â”€â†’ 4. DETAILED ENTITY VIEW
   â”‚                       â”‚
   â”‚                       â”œâ”€ Header with entity info
   â”‚                       â”œâ”€ Navigation Tabs:
   â”‚                       â”‚  â€¢ Overview
   â”‚                       â”‚  â€¢ Column Statistics
   â”‚                       â”‚  â€¢ Data Quality
   â”‚                       â”‚  â€¢ Patterns & Distributions
   â”‚                       â”‚  â€¢ Referential Integrity
   â”‚                       â”‚  â€¢ Candidate Keys
   â”‚                       â”‚  â€¢ PII Detection
   â”‚                       â”‚
   â”‚                       â””â”€â†’ [Back to Dashboard]
   â”‚
   â””â”€â†’ [View Job History] or Top Nav: [History]
       â”‚
       â””â”€â†’ 5. JOB HISTORY SCREEN
           â”‚
           â”œâ”€ Filter Panel (date, status, source)
           â”œâ”€ Jobs Table/List
           â””â”€â†’ [Click Job] â†’ Dashboard (read-only)
```

---

## Screen Descriptions

### 1. Home Screen (`/`)

**Purpose**: Landing page with quick access to profiling jobs and system overview

**Components**:
- **Quick Stats Section**
  - Total Jobs count
  - Active Jobs count
  - Completed Jobs count
  - Failed Jobs count

- **Recent Jobs List**
  - Shows last 5 profiling jobs
  - Displays: Job name, Status (badge), Date, Quick actions
  - Click on job â†’ Navigate to Dashboard

- **Actions**
  - "New Profiling Job" button â†’ Navigate to Configuration
  - "View All Jobs" link â†’ Navigate to History

**Navigation**:
- Click "New Profiling Job" â†’ `/configure`
- Click job row â†’ `/dashboard/{jobId}`
- Click "View All Jobs" â†’ `/history`
- NavBar "Home" â†’ Stay on `/`
- NavBar "History" â†’ `/history`

---

### 2. Configuration Screen (`/configure`)

**Purpose**: 3-step wizard to set up and start a new CSV profiling job

**Note**: Currently supports **Flat Files (CSV only)**. Database and Data Lake options are placeholders for future implementation.

#### **Step 1: Job Configuration & Data Source**

**Components**:
- **Job Name Input** (First field - Required)
  - Text input field with asterisk (*)
  - Placeholder: "e.g., Customer Data Profiling"
  - Must be filled before proceeding
  - Validation: Cannot be empty

- **Data Source Selection** (Second field)
  - **Connection Type Dropdown**:
    - Database (PostgreSQL, Oracle)
    - Data Lake API
    - Flat File (CSV, JSON, XML, Excel)
  - Currently only "Flat File" options are functional
  - Database and Data Lake shown but disabled for future

- **File Upload Area** (for Flat File)
  - Visual drag-and-drop zone with border styling
  - "Upload CSV Files" button â†’ Opens file picker (hidden input)
  - Accepts: `.csv` files only
  - Multiple file selection enabled
  - Shows "Uploading..." indicator during upload
  - Displays list of successfully uploaded files with:
    - File name
    - File size (KB)
  - Files can be removed from list (future enhancement)

**Functionality**:
- **Step Order**: User must enter job name FIRST before selecting data source
- Job name field is validated on blur (cannot be empty)
- Select data source from dropdown (Database/Data Lake/Flat File)
- Based on selection, appropriate upload/connection interface appears
- For Flat File:
  - User clicks "Upload CSV Files" button
  - File picker opens (multiple selection)
  - Selected files uploaded to backend `/api/v1/upload` endpoint
  - Each file uploaded individually via FormData
  - Backend returns `file_id` for each uploaded file
  - Frontend stores file metadata: `{name, size, path: file_id}`
  - Uploaded files list updated in real-time
  - Upload button disabled during upload process
  - Alert shown on success/failure

**Navigation**:
- "Cancel" â†’ Navigate to Home `/`
- "Next â†’" â†’ Step 2 (enabled only if job name filled AND at least 1 file uploaded)

---

#### **Step 2: File Parsing Configuration**

**Purpose**: Configure how uploaded files should be parsed

**Automatic Processing**:
- **File Encoding Detection**: Automatically detect and display file encoding (UTF-8, ISO-8859-1, etc.)
- **File Preview**: Show first 5-10 rows of the file to help user verify settings

**Components**:
- **Uploaded Files Section**
  - If files uploaded (from Step 1):
    - Heading: "Configure File Parsing (N files)"
    - For each file, display card with:
      - **File name** (bold) and size (KB)
      - **Detected Encoding**: Display auto-detected encoding (e.g., "UTF-8")
        - Option to manually override if needed
      - **Column Delimiter**: Dropdown with common options
        - Options: 
          - `,` (Comma)
          - `\t` (Tab)
          - `;` (Semicolon)
          - `|` (Pipe)
          - Custom (text input for other delimiters)
        - Default: Auto-detected or `,` (comma)
      - **Has Header Row** checkbox
        - Default: checked (yes)
        - Label: "First row contains column names"
      - **Preview** button: Show first few rows with detected/selected delimiter
    - **Add More Files** button â†’ Upload additional files
  - If no files uploaded:
    - Message: "No files uploaded yet. Go back to Step 1 to upload files."

**Functionality**:
- System automatically detects file encoding for each uploaded file
- User selects delimiter from common options dropdown (easier than typing)
- Option for custom delimiter if not in the list
- Toggle whether first row is header
- Preview data with selected settings to verify correctness
- **Add More Files**: Upload additional CSV files â†’ Added to list with own parsing config
- Each file can have different delimiter and header settings
- Validation on "Next": At least one file must be configured

**Current Implementation**:
- Delimiter dropdown with common options (to be implemented)
- Encoding auto-detection (to be implemented)
- Preview functionality (to be implemented)
- Currently: Manual text input for delimiter, single global setting

**Navigation**:
- "â† Back" â†’ Step 1 (returns to file upload screen)
- "Next â†’" â†’ Step 3 (enabled only if job name is filled)

---

#### **Step 3: Profiling Options**

**Purpose**: Configure what and how much to profile

**Components**:
- **Column Selection** (Radio buttons)
  - Option 1: "Profile All Columns" (default, selected)
  - Option 2: "Select Specific Columns" (disabled/grayed out - future feature)
  
- **Sample Size** (for CSV files)
  - Number input field
  - Label: "Sample Size (rows)"
  - Placeholder: "Leave empty to profile entire file"
  - Optional: If left empty, profiles all rows in file(s)

**Functionality**:
- **Column Selection**:
  - Currently only "Profile All Columns" is functional
  - "Select Specific Columns" is UI placeholder for future
- **Sample Size**:
  - Enter number of rows to limit profiling (e.g., 1000, 10000)
  - If empty/null: Profile all rows in the file(s)
  - Useful for large files to reduce processing time
- Validation on "Start Profiling":
  - Job name must be filled (checked in Step 2)
  - At least 1 file must be uploaded
  - Sample size must be positive integer if provided

**Navigation**:
- "â† Back" â†’ Step 2 (returns to dataset selection)
- "Start Profiling" â†’ 
  - Validates all inputs
  - Creates job via API call
  - On success: Navigate to `/dashboard/{jobId}`
  - On failure: Show error alert

**API Integration**:
- Button click triggers API call: `POST /api/v1/jobs`
- Request payload:
  ```json
  {
    "name": "Job Name from Step 2",
    "description": "Profiling N CSV file(s)",
    "file_paths": ["file_id_1", "file_id_2", ...],
    "csv_config": {
      "delimiter": ",",
      "encoding": "utf-8",
      "has_header": true
    },
    "treat_files_as_dataset": true,
    "sample_size": null or 1000
  }
  ```
- Response: `{ "job_id": "uuid", "name": "...", "status": "pending", ... }`
- On success: Navigate to `/dashboard/{job_id}`
- On error: Display error message from API

---

### 3. Dashboard Screen (`/dashboard/{jobId}`)

**Purpose**: Overview of profiling job results with dataset-level insights

**Components**:
- **Job Header**
  - Job name and status badge
  - Timestamp
  - Quick stats (Total Datasets, Total Rows, Total Columns)

- **Dataset Summary Cards**
  - One card per CSV file/dataset
  - Shows: Dataset name, Row count, Column count, Quality grade (Gold/Silver/Bronze/Red)
  - Click card â†’ Navigate to Entity View

- **Quality Distribution Chart**
  - Pie/donut chart showing quality grade distribution
  - Legend: Gold (90-100%), Silver (70-89%), Bronze (50-69%), Red (<50%)

- **Entity Table**
  - Lists all profiled datasets/entities
  - Columns: Entity Name, Row Count, Column Count, Quality Grade, Last Profiled
  - Click row â†’ Navigate to Entity View

**Functionality**:
- View job execution results
- Monitor data quality at dataset level
- Quick navigation to detailed entity profiles
- Visual quality distribution

**Navigation**:
- Click dataset card â†’ `/entity/{entityId}`
- Click entity table row â†’ `/entity/{entityId}`
- NavBar "Home" â†’ `/`
- Browser back â†’ Previous page

---

### 4. Entity View Screen (`/entity/{entityId}`)

**Purpose**: Detailed profiling results for a specific dataset/entity

**Components**:
- **Entity Header**
  - Entity name
  - Quality badge
  - Quick stats (Row Count, Column Count, Quality Score)

- **7 Tabs with Detailed Information**:

#### Tab 1: **Overview** (Dataset-Level Rules Summary)
- **Dataset Statistics**:
  - Total record count
  - Total column count
  - Dataset size (bytes/MB/GB)
  - Profiling timestamp and duration
- **Dataset-Level Data Quality Metrics**:
  - Overall completeness score (% non-null across all columns)
  - Overall data quality score (0-100)
  - Data quality grade badge (Gold/Silver/Bronze/Red)
  - PII risk score
- **Summary Visualizations**:
  - Quality score gauge
  - Completeness percentage bar
  - Grade indicator with color coding

#### Tab 2: **Column Statistics** (Attribute-Level Rules - All 8)
- **Table View** with expandable rows
- **For Each Column**:
  1. **Column Statistics**: Record count, null count, null %, unique count, distinct count, duplicate count
  2. **Data Type Analysis**: Inferred type, type consistency, format patterns, type mismatches
  3. **Numeric Analysis** (if numeric): Min, max, mean, median, std dev, variance, Q1, Q3, percentiles, outliers
  4. **String Analysis** (if text): Min/max/avg length, pattern frequency, character sets, leading/trailing spaces
  5. **Date/Time Analysis** (if date): Min/max date, range span, format patterns, timezone, invalid dates
  6. **Column-Level Data Quality**: Completeness %, validity %, consistency, conformity rate, quality score, quality grade
  7. **Value Distribution**: Frequency distribution, top N values, bottom N values, histogram bins, cardinality, mode
  8. **PII Detection**: Email, phone, SSN, credit card patterns, confidence score, risk level

#### Tab 3: **Data Quality** (Detailed Quality Breakdown)
- **Dataset-Level Quality Rules Results**:
  - Rule 1: Dataset Statistics (pass/fail)
  - Rule 2: Data Quality Metrics (scores and grades)
  - Rule 3: Referential Integrity (validation results)
  - Rule 4: Candidate Key Discovery (keys found)
- **Column-Level Quality Summary**:
  - List of columns with quality issues
  - Quality score distribution across columns
  - Top issues by severity
  - Quality trends (if multiple runs)
- **Visualizations**:
  - Quality score distribution histogram
  - Issue severity breakdown
  - Column quality heatmap

#### Tab 4: **Patterns & Distributions** (Value Analysis)
- **Per Column**:
  - Value distribution charts (histograms, bar charts)
  - Frequency tables (top N most/least common values)
  - Pattern frequency analysis
  - Format pattern detection (emails, phones, dates, etc.)
  - Character set distribution
  - Cardinality metrics
- **Visualizations**:
  - Interactive histograms
  - Frequency bar charts
  - Pattern pie charts

#### Tab 5: **Referential Integrity** (Cross-Column Analysis - Dataset-Level Rule 3)
- **Foreign Key Validation**:
  - Detected relationships between columns/tables
  - Orphan record detection
  - Missing reference validation
- **Cross-Table Consistency Checks**:
  - Value consistency across related columns
  - Referential integrity violations
- **Visualizations**:
  - Relationship diagrams
  - Violation count tables
  - Orphan record charts

#### Tab 6: **Candidate Keys** (Key Discovery - Dataset-Level Rule 4)
- **Single-Column Candidates**:
  - Columns with all unique values
  - Uniqueness percentage (100% or near-100%)
- **Composite Key Suggestions**:
  - Multi-column uniqueness combinations
  - Cardinality analysis
- **Primary Key Recommendations**:
  - Based on non-null + unique criteria
  - Key quality scores
- **Visualizations**:
  - Uniqueness distribution
  - Key candidate ranking
  - Composite key relationship diagrams

#### Tab 7: **PII Detection** (Sensitive Data - Attribute-Level Rule 8)
- **Per Column PII Results**:
  - PII patterns detected (email, phone, SSN, credit card, IP, address, names, DOB)
  - GDPR sensitive data categories
  - PII confidence score (0-100)
  - PII risk level (Low/Medium/High)
  - Sensitive data flags
- **Entity-Level PII Summary**:
  - Total PII columns detected
  - Overall PII risk score
  - Compliance indicators (GDPR, CCPA)
- **Visualizations**:
  - PII risk heatmap by column
  - Pattern detection confidence chart
  - Risk level distribution

**Functionality**:
- Deep dive into dataset quality
- Analyze column-level statistics
- Review quality rule violations
- Identify PII and sensitive data
- Export results (future)

**Navigation**:
- Click tab â†’ Switch view
- "â† Back to Dashboard" â†’ `/dashboard/{jobId}`
- NavBar "Home" â†’ `/`

---

### 5. History Screen (`/history`)

**Purpose**: View and manage all profiling jobs (past and present)

**Components**:
- **Jobs Table**
  - Columns: Job Name, Status, Source Type, Created Date, Actions
  - Status badges: Pending, Running, Completed, Failed
  - Sortable columns
  - Search/filter functionality (future)

- **Actions per Job**:
  - "View" â†’ Navigate to Dashboard
  - "Delete" â†’ Delete job (with confirmation)
  - "Re-run" â†’ Create new job with same config (future)

**Functionality**:
- View all historical profiling jobs
- Monitor job status
- Navigate to job results
- Delete old jobs
- Filter and search jobs (future)

**Navigation**:
- Click "View" â†’ `/dashboard/{jobId}`
- Click job row â†’ `/dashboard/{jobId}`
- NavBar "Home" â†’ `/`
- NavBar "New Profiling Job" â†’ `/configure`

---

## Navigation Bar (Persistent)

**Present on all screens**:
- **Left Side**:
  - "Home" link â†’ `/`
  - "History" link â†’ `/history`
  
- **Right Side**:
  - "New Profiling Job" button â†’ `/configure`

---

## Key Design Improvements

### **Updated Configuration Flow (3 Steps)**

**Step 1: Job Configuration & Data Source**
- âœ… **Job Name is now the FIRST field** - User enters job name before anything else
- âœ… **Data Source Dropdown** - Single dropdown to select: Database, Data Lake API, or Flat File
- âœ… File upload after source selection

**Step 2: File Parsing Configuration** 
- âœ… **Automatic Encoding Detection** - System detects file encoding (UTF-8, ISO-8859-1, etc.)
- âœ… **Delimiter Dropdown** - Easy selection from common delimiters:
  - `,` (Comma)
  - `\t` (Tab)
  - `;` (Semicolon)
  - `|` (Pipe)
  - Custom (manual input)
- âœ… **Has Header Row** checkbox
- âœ… **File Preview** - Verify parsing with sample rows

**Step 3: Profiling Options** (unchanged)
- Column selection
- Sample size configuration

### **Benefits of New Flow**
1. **Better Organization**: Job name upfront establishes context
2. **Easier Delimiter Selection**: Dropdown vs manual typing reduces errors
3. **Auto-Detection**: Encoding automatically detected, one less thing to configure
4. **Validation**: File preview helps verify settings before profiling
5. **Flexibility**: Each file can have different delimiter settings

---

## User Journey Examples

### **Journey 1: First-Time User - Profile CSV Files**
```
Home â†’ [+ New Profiling Job] â†’ 
Step 1: Enter job name: "Q4 Sales Data Analysis" (FIRST) â†’
        Select Data Source: [Flat File] from dropdown â†’
        Click [Upload CSV Files] â†’ 
        Select 3 files: customers.csv, orders.csv, products.csv â†’
        Files upload successfully (see file list with sizes) â†’ [Next] â†’
Step 2: File Parsing Configuration:
        System detects encoding: "UTF-8" for all files âœ“
        For customers.csv: Select delimiter from dropdown: "," (Comma) â†’
        Has header: âœ“ (checked) â†’
        Click [Preview] to verify â†’ looks good âœ“
        Configure orders.csv and products.csv similarly â†’ [Next] â†’
Step 3: [Profile All Columns] selected (default) â†’
        Sample Size: leave empty (profile all rows) â†’ [Start Profiling] â†’
Dashboard: 
  â€¢ Job created successfully
  â€¢ Navigate to Dashboard with job_id
  â€¢ See Dataset Summary: 3 entities, scores, row/column totals
  â€¢ View Quality Distribution Chart
  â€¢ Browse Entity List
  â€¢ Click "customers.csv" entity card â†’
Entity View: 
  â€¢ Tab through Overview, Column Statistics, Data Quality, etc.
  â€¢ Review column-level metrics
  â€¢ [Back to Dashboard] â†’ Review other entities
```

### **Journey 2: Returning User - Add More Files**
```
Home â†’ [+ New Profiling Job] â†’
Step 1: Enter job name: "Customer Analysis" â†’
        Select [Flat File] â†’ Upload 2 CSV files â†’ [Next] â†’
Step 2: System detects encoding for both files â†’
        Configure file1.csv: Delimiter dropdown = "," (Comma), Header = âœ“ â†’
        Configure file2.csv: Delimiter dropdown = ";" (Semicolon), Header = âœ“ â†’
        Click [Add More Files] â†’ Upload 1 more file â†’
        Now 3 files total, configure third file â†’
        [Next] â†’
Step 3: Enter sample size: 5000 (only profile first 5000 rows) â†’ [Start Profiling] â†’
Dashboard â†’ View results for sampled data
```

### **Journey 3: Review Job History**
```
Home â†’ Recent Jobs: Click "Q4 Sales Data Analysis (Nov 26)" â†’
Dashboard (read-only) â†’ Browse entities â†’
Click "orders.csv" â†’ Entity View â†’ Review Column Statistics tab â†’
[Back to Dashboard] â†’ [Export Report]
```

### **Journey 4: Explore Historical Jobs**
```
Home â†’ Top Nav: [History] â†’
History Screen:
  â€¢ See list of all profiling jobs
  â€¢ Filter by date range
  â€¢ Sort by completion time
  â€¢ Click [View] on "Customer Analysis - Nov 20" â†’
Dashboard: View completed job results
```

### **Journey 5: Large File with Sampling**
```
Home â†’ [+ New Profiling Job] â†’
Step 1: Upload large_dataset.csv (500MB, 10M rows) â†’ [Next] â†’
Step 2: Enter job name: "Large Dataset Sample" â†’ [Next] â†’
Step 3: Sample Size: 10000 (profile only 10K rows to save time) â†’ [Start Profiling] â†’
Dashboard: See results for 10K row sample â†’
Entity View: Analyze sampled data quality
```

## Current Implementation Status

### âœ… Fully Implemented
- All 5 screens with routing and navigation
- 3-step configuration wizard for CSV files
- CSV file upload integration with backend (`POST /api/v1/upload`)
- Job creation API integration (`POST /api/v1/jobs`)
- Navigation flow between all screens
- File upload with progress indication
- Job name validation
- Delimiter and header configuration
- Sample size option
- Responsive layout and styling

### ğŸ”„ Partially Implemented (Uses Mock Data)
- Dashboard visualizations (charts show static data)
- Entity summary cards (mock quality scores)
- Entity View tabs (mock column statistics)
- Quality grade calculations (Gold/Silver/Bronze badges are hardcoded)
- Backend API endpoints exist but profiling logic not implemented
- Job progress tracking UI ready but not connected to real-time updates

### ğŸ“‹ Future Enhancements (Not Yet Started)
- **Data Sources**:
  - Database connections (Oracle, PostgreSQL)
  - Data Lake API profiling
  - Additional file formats (JSON, XML, Excel)
- **Profiling Engine**:
  - Actual dataset-level rules implementation (4 rules)
  - Actual attribute-level rules implementation (8 rules per column)
  - Real quality score calculations
  - PII detection algorithms
  - Pattern recognition engine
  - Candidate key discovery
  - Referential integrity checks
- **UI Features**:
  - Real-time job progress tracking with WebSockets
  - Advanced filtering and search in entity list
  - Result export functionality (JSON/CSV/PDF)
  - Historical comparison between profiling runs
  - Column-specific profiling (select specific columns)
  - Custom quality rules configuration
  - Interactive chart drill-downs
  - Data lineage visualization
- **Backend Features**:
  - Oracle database integration for metadata storage
  - Migrate results from filesystem to Oracle
  - Job queue and background processing
  - Concurrent job execution
  - Job resumption after failures
  - Result retention and archival
- **Security & Management**:
  - User authentication and authorization
  - Role-based access control (RBAC)
  - Encrypted credential storage
  - Audit logging
  - API rate limiting

---

## API Endpoints Used

| Screen | Endpoint | Method | Purpose |
|--------|----------|--------|---------|
| Configuration Step 1 | `/api/v1/upload` | POST | Upload CSV files |
| Configuration Step 3 | `/api/v1/jobs` | POST | Create profiling job |
| Dashboard | `/api/v1/jobs/{jobId}` | GET | Get job details |
| Dashboard | `/api/v1/jobs/{jobId}/results` | GET | Get job results |
| Entity View | `/api/v1/results/entity/{entityId}` | GET | Get entity profile |
| History | `/api/v1/jobs` | GET | List all jobs |
| History | `/api/v1/jobs/{jobId}` | DELETE | Delete job |

---

## Data Flow

1. **User uploads CSV files** â†’ Files sent to backend â†’ File IDs returned
2. **User configures job** â†’ Job details captured in form state
3. **User starts profiling** â†’ Job creation request sent to backend â†’ Job ID returned
4. **Backend processes files** â†’ Profiling engine analyzes CSV data
5. **Results stored** â†’ Job status updated to "Completed"
6. **User views results** â†’ Frontend fetches results from API â†’ Displays in Dashboard/Entity View
